---
title: "k-means_methodes"
author: "Aziliz"
date: "2025-06-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Documentation à propos du clustering 

## <https://www.tidymodels.org/learn/statistics/k-means/>

\_Comment fonctionne le K-means ?\_
But : assigner chaque observation à un cluster k

	1- Spécifier le nombre de clusters 
	2- Chaque observation seule est assignée temporairement à un centroïde qui lui est proche 
	3- Le centroïde de chaque cluster est calculé selon l'ensemble des observations assignées au cluster
	
Nb : les centroïdes des clusters peuvent bouger => réassigner certaines observations à un centroïde devenu plus proche 

**= processus itératif de calcul des centroïdes et d'assignation des observation à celui qui leur est le plus proche**
**STOP : quand plus rien ne bouge**
**=> chaque observation est assignée à son cluster final !**

## Practical Guide To Cluster Analysis in R - Alboukadel Kassambara
## <https://xsliulab.github.io/Workshop/2021/week10/r-cluster-book.pdf>

\_**But du clustering**\_ = identifier des groupes d'objets similaires au sein d'un ensemble de données

* **K-means clustering** (MacQuenn, 1967) : algorithme d'apprentissage automatique le plus couramment utilisé pour partitionner un ensemble de données en un ensemble de k clusters.
Chaque cluster est représenté par son centre (*centroid*) = les principaux points assignés au cluster. - (chapitre 4)

Nb : utilisation des moyennes comme centre des clusters

\_Comment définir un cluster ?\_
Plusieurs méthodes et algorithmes possible mais le plus utilisé : 
* le Hartigan-Wong algorithme : intérieur d'un cluster = somme des carrés des distances euclidiennes entre les éléments et le centroïde correspondant

**Fonctionnement général du clustering (K-means) : assignation de chaque observation à un cluster --> calcul de la valeur principal de chaque cluster (centroid) --> vérification par l'algorithme que chaque observation est bien dans le cluster le plus proche d'un centroïde --> réassignation si ce n'est pas le cas --> modification des clusters --> nouveau calcul des centroïdes des clusters ...**

\_Etape 1\_ : indiquer le nombre de clusters qui seront générés dans le solution finale 

Comment choisir le bon nombre de clusters k que l'on veut au final? 
--> fonction **fviz_nbclust()** = solution efficace pour estimer le nombre optimal de clusters 

=> kmeans(df, 4 (= nombre de clusters k), nstart = n) 
Nb : mieux vaut indiquer un grand nombre (25, 50) pour *nstart* pour avoir une plus grande stabilité. 
*Nstart* : choix de n assignations de départ et R garde celle avec la variance la plus faible au sein du cluster 

Tirer un graphique de cela il faut utiliser le **PCA** (Principal Component Analysis) pour réduire le nombre de dimension des données qui ont plus de 2 variables. 
But : faire un graphique en se basant sur les coordonnées des deux composantes principales 

* **K-Medoids** = chaque cluster est représenté par l'un des points qu'il contient (*medoid*) 

Medioïde = objet au sein d'une grappe pour lequel la dissimilarité moyenne entre lui et tous les autres membres de la grappe est minimale

Algorithme principalement utilisé : **PAM** (Partitioning Around Medoids, Kaufman et Rousseeuw - 1990) - (chapitre 5)

Nb : pour les grands jeux de données mieux vaut privilégier l'utilisation de l'algorithme **CLARA** (chapitre 6)

Comment fonctionne cet algorithme ? 
 1. Sélectionne k objets qui deviendront les médioïdes 

 2. Calcul de la matrice de dissimilarité si elle n'a pas été fournie
 
 3. Affecte chaque objet à son médioïde le plus proche 

 4. Pour chaque grappe, recherche si l'un des objets de la grappe diminue le coefficient de dissimilarité moyen ; si c'est le cas, sélectionne l'entité qui diminue ce coefficient.
 
 5. Si au moins un médoïde a changé, retour à (3), sinon fin l'algorithme.

L'algorithme PAM demande à l'utilisateur de connaître les données et d'indiquer le nombre approprié de grappes à produire.
Ce nombre peut être estimé à l'aide de la fonction **fviz_nbclust** [dans le paquetage R de *factoextra*].

Après avoir effectué le regroupement PAM, la fonction R **fviz_cluster()** [paquet factoextra] peut être utilisée pour visualiser les résultats. Le format est **fviz_cluster(pam.res)**, où pam.res est le résultat du PAM.

# Objectifs avec la méthode du clustering 

1- Grouper les communes par spécialité de production agricole

2- Comprendre la signification de ces groupes (comment l'algorithme en est arrivée là)

3- Construire une carte des communes représentant les spécifications (par un code couleur par exemple)

# Premières approches du clustering 

## Préparation de l'environnement 
```{r, echo = FALSE}
# Clean memory 
rm(list=ls())
gc()

# Load package
if (!require("pacman")) install.packages("pacman")
pacman::p_load(cluster, factoextra, dplyr, here, tidyr, tibble, FactoMineR)

# List directories 
dir <- list()
dir$root <- here()
dir$data <- here(dir$root, "data")
dir$raw <- here(dir$data, "raw")
dir$derived <- here(dir$data, "derived")
dir$final <- here(dir$data, "final")
dir$script <- here(dir$root, "script")
dir$output <- here(dir$root, "output")
dir$faostat_data <- here(dir$root, "faostat_data")

# Create non existing directories
lapply(dir, function(i) dir.create(i, recursive = T, showWarnings = F))
```

## Détermination du nombre de clusters optimal 
NB : subjectif, dépend de la méthode employées pour mesurer les similarités entre les paramètres : méthodes directes (`elbow` and `silhouette`) ou méthode de test statistique (`gap statistic`)

- `Elbow method` : faire varier le nombre de clusters k entre 1 et 10 et pour chaque k le WSS (mesure la compacité du regroupement). 
Le but est ensuite d'en tirer un graphique, k-optimal sera indiqué par une courbure, un "coude". 

- `Average silhouette method` : mesure la qualité d'un clustering (un indice élevé indique l'optimalité du clustering)
A l'inverse de "Elbow", ici, le k-optimal sur le graphique sera le maximum de la courbe.

- `Gap statistic method` : comapraison des variations à l'intérieur des "grappes" pour différentes valeurs de k avec leurs valeurs attendues dans le cadre d'une distribution de référence des valeurs.
k-optimal = valeur qui maximise le "gap statistic"

Dans notre cas, manuellement ça donne ça : 
```{r, eval = FALSE}
silhouette <- fviz_nbclust(commune_scaled, clara, method = "silhouette")+
  theme_classic()
print(silhouette)
```
 Maintenant, le but est de l'automatiser pour qu'il soit calculer dans le script et que la suite du script s'en serve automatiquement pour construire les clusters.


## Tentative n°1 - à partir exemple du manuel d'Alboukadel Kassambara : ECHEC 
Raisonnement : 
- nettoyer les données
- garder seulement les données numériques 
- standardiser les données 
- appliquer le clustering CLARA 
- visualiser le clustering 

Problème : jeu de données mixtes : variables numériques et catégoriques OR : PAM et CLARA ne traitent que de données numériques pour construire une matrice à l'aide d'un système vectoriel

Lorsqu'on tente de forcer les variables numériques, on obtient bien plusieurs clusters mais on se siat à quoi ils renvoient. De plus, ils ne sont pas clairs.

```{r - essai_1, eval = FALSE}
RPG_R53 <- readRDS(here(dir$final, "RPG_Aggregated_Britanny.rds"))
RPG_R53_clean <- na.omit(RPG_R53)

# Garder seulement les colonnes numériques
RPG_R53_numeric <- RPG_R53_clean[, sapply(RPG_R53_clean, is.numeric)]

# Standardiser les données
RPG_R53_scaled <- scale(RPG_R53_numeric)

# Appliquer le clustering CLARA (par exemple k = 3)
clara_res <- clara(RPG_R53_scaled, 3, samples = 50, pamLike = TRUE)

# Visualiser les clusters
print(fviz_cluster(clara_res, 
             data = RPG_R53_scaled,
             ellipse.type = "t",
             geom = "point", pointsize = 1,
             ggtheme = theme_classic()))
```

## Tentative n°2 : tableau en longueur plutôt qu'en largeur : colonnes spécifiques pour chaque groupe de culture ? 

```{r}
RPG_R53_wide <- readRDS(here(dir$final, "RPG_Aggregated_Brittany_wide.rds"))
columns_to_select <- paste0("parcel_cult_code_group_perc_G", 1:25)

RPG_R53_clean_2015 <- RPG_R53_wide %>%
  filter(year == 2015) %>%
  na.omit()
          
commune_scaled <- RPG_R53_clean_2015 %>%
  select(all_of(columns_to_select)) %>%
  scale()

# Détermination du nombre de cluster optimal
sil_data <- fviz_nbclust(commune_scaled, clara, method = "silhouette", correct.d=TRUE)
optimal_k <- which.max(sil_data$data$y)

# Clustering final
set.seed(123)
clustering <- clara(commune_scaled, k = optimal_k, samples = 50, pamLike = TRUE)

# Graphique
fviz_cluster(clustering, 
             data = commune_scaled,
             ellipse.type = "t", 
             geom = "point", pointsize = 1,
             ggtheme = theme_minimal())
```

Si on essaie d'analyser un peu ce graphique des clusters : on note que le cluster 3 semble plus "compact" (on le voit à l'ellipse) et moins dispersé ce qui pourrait indiquer qu'il regroupe des observations avec des caractéristiques proches à l'inverse des clusters 1 et 2 qui semblent potentiellement plus hétérogènes. 

Dim1 et Dim2 correspondent aux deux premières composantes principales. 
La première dimension explique 9,6% de la variance totale des données alors que la deuxième en explique 7,1%.
DONC : les points du plan sont des projections des observations initiales dans l'espace des deux dimensions principales. 

Il est possible d'étudier quelles variables influencent le plus Dim1 et Dim2, ce qui peut nous renseigner davantage sur la construction de ces clusters et de la visualisation finale. 

```{r}
res.pca <- PCA(commune_scaled, graph = FALSE)

# Contributions des variables à Dim1 et Dim2
fviz_contrib(res.pca, choice = "var", axes = 1) 
fviz_contrib(res.pca, choice = "var", axes = 2) 
```
Ainsi, ces histogrammes nous montre que les groupes de cultures G18, G1, G2, G16 et G5 sont ceux qui influencent le plus Dim1 alors que pour Dim2 c'est très largement G19 puis en plus faible proportion G3, G5, G25 ou G11. 

Notons que nous avons ici de faibles proportions de variance expliquée par Dim1 et Dim2, cela s'explique par la multiplicité des dimensions qui existent dans notre cas. Je tend à supposer donc que l'interprétation en cluster en tout cas par la visualisation n'est peut-être pas tant représentative de l'ensemble des données que ça...

```{r}
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))
```

Pour expliquer au moins 50% de la variance totale, il faudrait 10 dimensions. 

D'autre part, ce bout de code principal permet d'obtenir une visualisation graphique des clusters. Cependant, il manque d'informations sur les regroupements réalisés, par exemple selon quelle type de culture dominante ... 

Tentative n°1 pour pallier le problème : afficher des labels selon le groupe de culture dominant sur les communes sur le graphique mais cela s'est avéré inefficient au vue du nombre trop important d'observations.

```{r}
RPG_R53_wide <- readRDS(here(dir$final, "RPG_Aggregated_Brittany_wide.rds"))
columns_to_select <- paste0("parcel_cult_code_group_perc_G", 1:25)
RPG_R53_clean <- na.omit(RPG_R53_wide)

# Générer labels de groupe de culture dominant dans les communes
RPG_R53_clean <- RPG_R53_clean %>%
  rowwise() %>%
  mutate(dominant_production = paste0("G", which.max(c_across(starts_with("parcel_cult_code_group_perc")))))
          
commune_scaled <- RPG_R53_clean %>%
  select(all_of(columns_to_select)) %>%
  scale()

# Détermination du nombre de cluster optimal
sil_data <- fviz_nbclust(commune_scaled, clara, method = "silhouette", correct.d=TRUE)
optimal_k <- which.max(sil_data$data$y)

# Clustering final
set.seed(123)
clustering <- clara(commune_scaled, k = optimal_k, samples = 50, pamLike = TRUE)

# Graphique
fviz_cluster(clustering, 
             data = commune_scaled,
             ellipse.type = "t", 
             geom = "point", pointsize = 1,
             ggtheme = theme_minimal()) +
  geom_text(aes(label = RPG_R53_clean$dominant_production), size = 3, hjust = 0.5, vjust = -1)
```

Vu que je n'arrive pas à faire ressortir sur le graphique les cultures dominantes. Il me semble intéressant d'analyser les caractéristique de chaque cluster. 

## Etude de la distribution moyenne des types de production dans chaque cluster. 

```{r}
# Ajouter les clusters aux données originales
RPG_R53_clean <- cbind(RPG_R53_clean, cluster = clustering$clustering)
head(clustering)
# Calculer les moyennes des variables pour chaque cluster
cluster_summary <- RPG_R53_clean %>%
  group_by(cluster) %>%
  summarise(across(starts_with("parcel_cult_code_group_perc"), mean, na.rm = TRUE))
  
print(cluster_summary)
```
 
 Type de cultures dominantes pour le cluster 1 : 
 
 - G19 : 0,287
 - G2 : 0,272
 - G1 : 0,163
 
 Pour le cluster 2 : 
 
 - G19 : 0,185 
 - G2 : 0,196
 - G1 : 0,111
 - G18 : 0,261
 
 Pour le cluster 3 : 
 
 - G25 : 0,394
 - G19 : 0,110
 - G2 : 0,141

Possibilité d'émettre l'hypothèse que les communes concernées par le cluster 3 paratgent des pratiques agricoles ou ont des conditions extérieures qui favorisent G25, 19 et 2? Le cluster 3 semble davantage spécialisé que les autres avec une production de G25 à hauteur de 39,4%.

## Analyse des observations individuelles de chaque cluster (fastidieux)

```{r}
# Filtrer les observations d'un cluster spécifique
cluster_1_obs <- RPG_R53_clean %>%
  filter(cluster == 1)
print(cluster_1_obs)

cluster_2_obs <- RPG_R53_clean %>%
  filter (cluster == 2)
print(cluster_2_obs)

cluster_3_obs <- RPG_R53_clean %>%
  filter (cluster == 3)
print(cluster_3_obs)
```


Essayer refaire cluster avec juste 2015 
ou prendre la moyenne en surface par culture sur les 15 années d'observations 

Faire la carte : créer un nouv tableau de données --> issus du clustering (anom commune, code commune, années, cluster, perc) - apparayé avec tableau de données des communes avec geometry (shape file)
regarder ce qui est très faible --> means sur toutes les colonnes de groupes de cultures à corriger avec une année 
